{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# How Does BERTScore Work?\n",
        "\n",
        "## A step-by-step guide:\n",
        "\n",
        "This simplified guide is designed to give you a brief overview of the BERTScore metric.  It's not 100% accurate and may give some slightly different scores to the original, but serves as a quick demonstration.\n",
        "\n",
        "**If you're running this on Google Colab, it will run faster if you select \"Runtime\" -> \"Change runtime type\" -> \"T4 GPU\" -> \"Save\".**\n",
        "\n",
        "### Step 1: Let's install a couple of libraries which will help us later on"
      ],
      "metadata": {
        "id": "Ex7vh24BTHvG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install transformers\n",
        "!pip install torch",
        "!pip install numpy",
        "!pip install matplotlib"
      ],
      "metadata": {
        "id": "l8ctM-JRmfFa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Step 2: Let's import the libraries we'll need and define the sentences we're going to compare"
      ],
      "metadata": {
        "id": "VV1hgy27UG8x"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoTokenizer, AutoModel\n",
        "import torch\n",
        "from statistics import mean\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from math import log\n",
        "\n",
        "sentence1 = \"the cat sat on the mat\"\n",
        "sentence2 = \"the the the the the the\""
      ],
      "metadata": {
        "id": "e7X0j-mgmiWc"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Step 3: Tokenise the sentences into a form BERTScore can use\n",
        "\n",
        "You will note that we are converting these to pytorch tensors for use with a GPU, as well as padding them out. The first eight tokens in the tensors represent the strings we're working with."
      ],
      "metadata": {
        "id": "1vaPBMQhU0Sv"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "R1m3rRQjbjLb"
      },
      "outputs": [],
      "source": [
        "# Tokenise the text\n",
        "tokeniser = AutoTokenizer.from_pretrained(\"distilbert-base-uncased\")\n",
        "\n",
        "\n",
        "# BERTScore adds a mask here - when used on a GPU, padding tokens are needed.\n",
        "# We will have to remove these masks later, in order to calculate the score.\n",
        "# Is it just me, or does flooding the GPU with zeros to make your code work\n",
        "#   seem a bit... hacky?\n",
        "sentence1Tokens = tokeniser.encode(sentence1,\n",
        "                                   return_tensors=\"pt\",\n",
        "                                   padding=\"max_length\")\n",
        "sentence2Tokens = tokeniser.encode(sentence2,\n",
        "                                   return_tensors=\"pt\",\n",
        "                                   padding=\"max_length\")\n",
        "\n",
        "print(sentence1Tokens)\n",
        "print(sentence2Tokens)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Step 4: Use a BERT model to encode the tokens"
      ],
      "metadata": {
        "id": "W_2joeMiU6UW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Encode the tokens\n",
        "model = AutoModel.from_pretrained(\"distilbert-base-uncased\")\n",
        "\n",
        "sentence1EncTokens = model(input_ids=sentence1Tokens)\n",
        "sentence2EncTokens = model(input_ids=sentence2Tokens)\n",
        "\n",
        "print(sentence1EncTokens)\n",
        "print(sentence2EncTokens)"
      ],
      "metadata": {
        "id": "fHqGPXMFnx_1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Step 5: Calculate the cosine similarity between the two sentences"
      ],
      "metadata": {
        "id": "divVpN6CVzuX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Calculate cosine similarity:\n",
        "#  (This is a cutdown version of the actual method used in BERTScore).\n",
        "def greedy_cos_idf(s1Embeddings,\n",
        "                   s2Embeddings):\n",
        "    \"\"\"\n",
        "    Compute greedy matching based on cosine similarity.\n",
        "\n",
        "    Args:\n",
        "        s1Embeddings (torch.Tensor): embeddings of reference sentences.\n",
        "        s2Embeddings (torch.Tensor): embeddings of candidate sentences.\n",
        "    Returns:\n",
        "        torch.Tensor: the cosine similarity.\n",
        "    \"\"\"\n",
        "    s1Embeddings.div_(torch.norm(s1Embeddings, dim=-1).unsqueeze(-1))\n",
        "    s2Embeddings.div_(torch.norm(s2Embeddings, dim=-1).unsqueeze(-1))\n",
        "\n",
        "    cosSimilarity = torch.bmm(s2Embeddings, s1Embeddings.transpose(1, 2)) # loss\n",
        "    return cosSimilarity\n",
        "\n",
        "cosSimilarity = greedy_cos_idf(sentence1EncTokens.last_hidden_state,\n",
        "                               sentence2EncTokens.last_hidden_state)\n",
        "\n",
        "# This figure shows how padding tokens score much higher than others.\n",
        "plt.plot(cosSimilarity.squeeze(0).detach().cpu().numpy())\n",
        "\n",
        "# This figure shows how attention works in padding tokens (similarity ~ 1).\n",
        "plt.imshow(cosSimilarity.squeeze(0).detach().cpu().numpy())\n",
        "\n",
        "# This figure zooms in on the tokens from our sentences, plus the first few padding tokens.\n",
        "plt.imshow(cosSimilarity.squeeze(0)[:10,:10].detach().cpu().numpy())\n",
        "\n"
        "print(cosSimilarity)"
      ],
      "metadata": {
        "id": "iCrUZ1-TogBf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Step 6: Remove the masks we added earlier"
      ],
      "metadata": {
        "id": "5q_jUDCuYBlb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# BERTScore Mask magic\n",
        "# BERTScore adds masks because when used on a GPU, padding tokens are needed.\n",
        "# We then have to remove these masks here, in order to calculate the score.\n",
        "sentence1Masks = torch.where(sentence1Tokens == 0, 0, 1)\n",
        "sentence2Masks = torch.where(sentence2Tokens == 0, 0, 1)\n",
        "masks = torch.matmul(sentence2Masks.float().T, sentence1Masks.float())\n",
        "masks = masks.float().to(cosSimilarity.device)\n",
        "\n",
        "cosSimilarity = cosSimilarity * masks"
      ],
      "metadata": {
        "id": "wytWTbU7vqVK"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Step 7: Calculate the Precision-Recall-F1 score"
      ],
      "metadata": {
        "id": "r-njxbSniL0H"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Calculate Precision-Recall-F1\n",
        "wordPrecision = cosSimilarity.max(dim=2)[0]\n",
        "wordRecall = cosSimilarity.max(dim=1)[0]\n",
        "\n",
        "# Calculate Inverse Document Frequency (IDF)\n",
        "# This goes beyond what we're discussing today, the final result of this\n",
        "#   calculation is around 9 for the data we're using.\n",
        "idf = 9\n",
        "\n",
        "precision = wordPrecision.sum(dim=1) / idf\n",
        "recall = wordRecall.sum(dim=1) / idf\n",
        "f1 = (2 * precision * recall / (precision + recall))\n",
        "\n",
        "# Just handling out-of-dictionary errors.\n",
        "f1 = f1.masked_fill(torch.isnan(f1), 0.0)\n",
        "\n",
        "print(f\"Precision: {precision.item()}\")\n",
        "print(f\"Recall: {recall.item()}\")\n",
        "print(f\"F1: {f1.item()}\")"
      ],
      "metadata": {
        "id": "DFPuIkeVpbby"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Comparison with the official implementation of BERTScore\n",
        "\n",
        "The example we've been working on so far has been using a number of close approximations to emulate how BERTScore works without diving too deep into the mathematics behind the metric.  Here's an example script which uses the BERTScore metric, as implemented by HuggingFace to perform the same task."
      ],
      "metadata": {
        "id": "GDlMlQdFiciV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# For Comparison, using the actual BERTScore metric\n",
        "!pip install evaluate\n",
        "!pip install bert_score\n",
        "import evaluate\n",
        "\n",
        "sentences = [[\"the cat sat on the mat\"], [\"the the the the the the\"]]\n",
        "\n",
        "def getMetric(metricName: str) -> evaluate.EvaluationModule:\n",
        "    \"\"\" Gets a metric from HuggingFace's Evaluate API.\n",
        "        Their network can get flaky when busy, so prepare for an Exception.\n",
        "    Args:\n",
        "        metricName (str): the name of the metric to use.\n",
        "    Returns:\n",
        "        EvaluationModule: the metric.\n",
        "    \"\"\"\n",
        "    try:\n",
        "        return evaluate.load(metricName)\n",
        "    except Exception as e:\n",
        "        print(f\"could not access HuggingFace {metricName}\")\n",
        "        raise e\n",
        "\n",
        "def getBSf1(metric: evaluate.EvaluationModule,\n",
        "            predictions: list,\n",
        "            references: list) -> float:\n",
        "    \"\"\" Get's BERTScore's F1 value.\n",
        "    Args:\n",
        "        metric (evaluate.EvaluationModule): the metric to use.\n",
        "        predictions (list(str)): any prediction sentences to compare.\n",
        "        references (list(str)): any reference sentences to compare.\n",
        "    Returns:\n",
        "        float: the F1 score.\n",
        "    \"\"\"\n",
        "    f1 = metric.compute(predictions=predictions,\n",
        "                        references=references,\n",
        "                        model_type=\"distilbert-base-uncased\", #  Pick a BERT.\n",
        "                        lang=\"en\",\n",
        "                        device=\"cuda\",\n",
        "                        batch_size=48)[\"f1\"]\n",
        "    return f1\n",
        "\n",
        "bERTScore = getMetric(\"bertscore\")\n",
        "f1 = getBSf1(bERTScore,\n",
        "             sentences[0],\n",
        "             sentences[1])\n",
        "\n",
        "print(f\"F1: {f1[0]}\")"
      ],
      "metadata": {
        "id": "apJOKZ1k2UEw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Authors: Jesse Phillips <j.m.phillips@lancaster.ac.uk>, and Stephen Mander <s.mander3@lancaster.ac.uk>.\n",
        "\n",
        "Presented as part of a talk to the UCREL NLP group at Lancaster University, March 2024."
      ],
      "metadata": {
        "id": "fifHtODvoeds"
      }
    }
  ]
}
